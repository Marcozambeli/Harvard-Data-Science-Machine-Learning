Key points
Bayes' rule: 
ğ‘(ğ‘¥)=ğ‘ƒğ‘Ÿ(ğ‘Œ=1|ğ‘‹=ğ‘¥)=ğ‘“ğ‘‹|ğ‘Œ=1(ğ‘‹)ğ‘ƒğ‘Ÿ(ğ‘Œ=1)ğ‘“ğ‘‹|ğ‘Œ=0(ğ‘‹)ğ‘ƒğ‘Ÿ(ğ‘Œ=0)+ğ‘“ğ‘‹|ğ‘Œ=1(ğ‘‹)ğ‘ƒğ‘Ÿ(ğ‘Œ=1) 
with  ğ‘“ğ‘‹|ğ‘Œ=1  and  ğ‘“ğ‘‹|ğ‘Œ=0  representing the distribution functions of the predictor  ğ‘‹  for the two classes  ğ‘Œ=1  and  ğ‘Œ=0 . 

The Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman External link.
Code
# Generating train and test set
library("caret")
data("heights")
y <- heights$height
set.seed(2)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)

# Estimating averages and standard deviations
params <- train_set %>%
 group_by(sex) %>%
 summarize(avg = mean(height), sd = sd(height))
params

# Estimating the prevalence
pi <- train_set %>% summarize(pi=mean(sex=="Female")) %>% pull(pi)
pi

# Getting an actual rule
x <- test_set$height
f0 <- dnorm(x, params$avg[2], params$sd[2])
f1 <- dnorm(x, params$avg[1], params$sd[1])
p_hat_bayes <- f1*pi / (f1*pi + f0*(1 - pi))
