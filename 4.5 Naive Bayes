Key points
Bayes' rule: 
𝑝(𝑥)=𝑃𝑟(𝑌=1|𝑋=𝑥)=𝑓𝑋|𝑌=1(𝑋)𝑃𝑟(𝑌=1)𝑓𝑋|𝑌=0(𝑋)𝑃𝑟(𝑌=0)+𝑓𝑋|𝑌=1(𝑋)𝑃𝑟(𝑌=1) 
with  𝑓𝑋|𝑌=1  and  𝑓𝑋|𝑌=0  representing the distribution functions of the predictor  𝑋  for the two classes  𝑌=1  and  𝑌=0 . 

The Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman External link.
Code
# Generating train and test set
library("caret")
data("heights")
y <- heights$height
set.seed(2)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)

# Estimating averages and standard deviations
params <- train_set %>%
 group_by(sex) %>%
 summarize(avg = mean(height), sd = sd(height))
params

# Estimating the prevalence
pi <- train_set %>% summarize(pi=mean(sex=="Female")) %>% pull(pi)
pi

# Getting an actual rule
x <- test_set$height
f0 <- dnorm(x, params$avg[2], params$sd[2])
f1 <- dnorm(x, params$avg[1], params$sd[1])
p_hat_bayes <- f1*pi / (f1*pi + f0*(1 - pi))
