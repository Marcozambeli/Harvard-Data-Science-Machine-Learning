Key points
Classification trees, or decision trees, are used in prediction problems where the outcome is categorical. 
Decision trees form predictions by calculating which class is the most common among the training set observations within the partition, rather than taking the average in each partition.
Two of the more popular metrics to choose the partitions are the Gini index and entropy.
Gini(ğ‘—)=âˆ‘ğ‘˜=1ğ¾ğ‘Ì‚ ğ‘—,ğ‘˜(1âˆ’ğ‘Ì‚ ğ‘—,ğ‘˜) 
entropy(ğ‘—)=âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘Ì‚ ğ‘—,ğ‘˜ğ‘™ğ‘œğ‘”(ğ‘Ì‚ ğ‘—,ğ‘˜),with 0Ã—log(0)defined as 0 
Pros: Classification trees are highly interpretable and easy to visualize.They can model human decision processes and donâ€™t require use of dummy predictors for categorical variables.
Cons: The approach via recursive partitioning can easily over-train and is therefore a bit harder to train than. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. 
Code
# fit a classification tree and plot it
train_rpart <- train(y ~ .,
              method = "rpart",
              tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
              data = mnist_27$train)
plot(train_rpart)

# compute accuracy
confusionMatrix(predict(train_rpart, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]
